{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test FastText with LDA labels and then predict class for test set\n",
    "Split doc2vec training into labelled train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X = xrange(len(docs))\n",
    "y = xrange(len(docs))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "train_file = codecs.open('/home/daniel/Desktop/fastText/train.txt', 'w', 'utf-8')\n",
    "for i in X_train:\n",
    "    label =  ' __label__Topic' + str(np.argmax(topics_dist[i,:])) + '\\n'\n",
    "    train_file.write(docs[i].strip() + label)\n",
    "train_file.close()\n",
    "\n",
    "test_file = codecs.open('/home/daniel/Desktop/fastText/test.txt', 'w', 'utf-8')\n",
    "for i in X_test:\n",
    "    label =  ' __label__Topic' + str(np.argmax(topics_dist[i,:])) + '\\n'\n",
    "    test_file.write(docs[i].strip() + label)\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script bash\n",
    "cd ~/ms/Daniel/fastText/\n",
    "./fasttext supervised -dim 150 -lr 0.1 -wordNgrams 3 -input ~/Desktop/fastText/train.txt -output ~/Desktop/fastText/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script bash\n",
    "cd ~/ms/Daniel/fastText/\n",
    "./fasttext test ~/Desktop/fastText/train.bin ~/Desktop/fastText/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train FastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script bash\n",
    "cd ~/ms/Daniel/fastText\n",
    "#./fasttext skipgram -dim 150 -minCount 1 -input /home/daniel/s2ds/Data/04_doc2vecTrainingDataFiltered.txt -output /home/daniel/Desktop/fastText/model\n",
    "./fasttext supervised -dim 100 -input /home/daniel/s2ds/Data/05_fastTextConvLabelled.txt -output /home/daniel/Desktop/fastText/modelLabel\n",
    "#./fasttext supervised -dim 100 -wordNgrams 3 -input /home/daniel/s2ds/Data/05_clientMessagesFilteredfastTextLabelled.txt -output /home/daniel/Desktop/fastText/modelLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the fastText model and make a (row) normalized word embedding from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "%cd ~/Desktop/fastText/\n",
    "word_vects = pd.read_table('modelLabel.vec', index_col=0, header=None, sep=' ')   \n",
    "# last col is all nans, because an added extra space, get rid off it\n",
    "word_vects = word_vects.iloc[:, :word_vects.shape[1]-1]\n",
    "# normalize all word vects to one\n",
    "# word_norms = np.linalg.norm(word_vects.values, axis=1)\n",
    "# data = word_vects.values / word_norms[:, np.newaxis]\n",
    "# word_vects = pd.DataFrame(data, index=word_vects.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if the word embedding make sence, by looking for closest vectors in the vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_norms = np.linalg.norm(word_vects.values, axis=1)\n",
    "data = word_vects.values / word_norms[:, np.newaxis]\n",
    "word_vects = pd.DataFrame(data, index=word_vects.index)\n",
    "def get_nearest_vect(word):\n",
    "    query = word_vects.loc[word].values/np.linalg.norm(word_vects.loc[word].values)\n",
    "    cosines = np.dot(word_vects, query)\n",
    "    # get second closest vector (becuase the closest will be the query)\n",
    "    sorted_cosines = np.argsort(cosines)\n",
    "    cos_counter = 0\n",
    "    for i, cos in enumerate(cosines[sorted_cosines][::-1]):\n",
    "        if cos_counter == 1 and cos != np.nan:\n",
    "            # tricky indexing because we reversed the sorted list above\n",
    "            max_cosine = sorted_cosines[-i-1]\n",
    "            break\n",
    "        if not np.isnan(cos):\n",
    "            cos_counter += 1\n",
    "\n",
    "    return word_vects.index[max_cosine], cosines[max_cosine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_nearest_vect('order')\n",
    "#np.linalg.norm(word_vects.loc['receive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train TF-IDF on data doc2vecTraingingData and collapse it down to a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data_folder = '/home/daniel/s2ds/Data/'\n",
    "input_file = '05_fastTextConv.txt'\n",
    "input_file = codecs.open(os.path.join(data_folder, input_file), 'rU', 'utf-8')\n",
    "docs = []\n",
    "for line in input_file:\n",
    "    docs.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=.95, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "tfidf_words = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = np.mean(tfidf.toarray(), axis=0)\n",
    "tfidf_dict = {}\n",
    "for i, t in enumerate(tfidf):\n",
    "    tfidf_dict[tfidf_words[i]] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_x = sorted(tfidf_dict.items(), key=operator.itemgetter(1))\n",
    "#sorted_x[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate simple client vectors with averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client_file = '05_clientMessagesFilteredfastText.txt'\n",
    "client_file = codecs.open(os.path.join(data_folder, client_file), 'rU', 'utf-8')\n",
    "summary_file = os.path.join(data_folder, 'client_agent_summary3.csv')\n",
    "summary = pd.read_csv(summary_file, index_col=0)\n",
    "# get the position of each sentence in a given conversation: matches length of client_file\n",
    "sent_pos = summary.convPos[~pd.isnull(summary.linePosFiltered)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sent_comp_vect2(embedding_df, sent):\n",
    "    sent = sent.split()\n",
    "    sent_vect = None\n",
    "    w_count = 0\n",
    "    for word in sent:\n",
    "        try:\n",
    "            word_vect = embedding_df.loc[word].values\n",
    "            if sent_vect is None:\n",
    "                sent_vect = word_vect\n",
    "            else:\n",
    "                np.add(sent_vect, word_vect)\n",
    "            w_count += 1\n",
    "        except:\n",
    "            pass\n",
    "    # return vect with zeros if None\n",
    "    if sent_vect is None:\n",
    "        return np.zeros(embedding_df.shape[1])\n",
    "    else:\n",
    "        # an averaged vector\n",
    "        return sent_vect/w_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desktop = '/home/daniel/Desktop/fastText/'\n",
    "client_sentences = 'clientMessagesVectors_Labelled_nomemory.csv'\n",
    "client_sentences = codecs.open(os.path.join(desktop, client_sentences), 'w', 'utf-8')\n",
    "# keeping track of the current conversation\n",
    "prev_sent_pos = 100\n",
    "# prev sentences' weigh less\n",
    "past_w = .7\n",
    "curr_w = 1\n",
    "pos_counter = 1\n",
    "for i, line in enumerate(client_file):\n",
    "    if i % 10000 == 0:\n",
    "        print i\n",
    "    sent_vect = get_sent_comp_vect2(word_vects, line)\n",
    "    line = str(i) + ',' + ','.join(map(str, sent_vect)) + '\\n'\n",
    "    client_sentences.write(line) \n",
    "    prev_sent_pos = sent_pos[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate time-aware sentence vectors from the filtered client corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sent_comp_vect(tfidf_dict, embedding_df, sent):\n",
    "    sent = sent.split()\n",
    "    sent_vect = None\n",
    "    w_count = 0\n",
    "    for word in sent:\n",
    "        try:\n",
    "            word_vect = embedding_df.loc[word].values\n",
    "            word_w = tfidf_dict[word]\n",
    "            if sent_vect is None:\n",
    "                sent_vect = word_vect * word_w\n",
    "            else:\n",
    "                np.add(sent_vect, word_vect * word_w)\n",
    "            w_count += 1\n",
    "        except:\n",
    "            pass\n",
    "    # return vect with zeros if None\n",
    "    if sent_vect is None:\n",
    "        return np.zeros(embedding_df.shape[1])\n",
    "    else:\n",
    "        # an averaged vector\n",
    "        return sent_vect/w_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "desktop = '/home/daniel/Desktop/fastText/'\n",
    "client_sentences = 'clientMessagesVectors_Labelled.csv'\n",
    "client_sentences = codecs.open(os.path.join(desktop, client_sentences), 'w', 'utf-8')\n",
    "# keeping track of the current conversation\n",
    "prev_sent_pos = 100\n",
    "# prev sentences' weigh less\n",
    "past_w = .7\n",
    "curr_w = 1\n",
    "pos_counter = 1\n",
    "for i, line in enumerate(client_file):\n",
    "    if i % 10000 == 0:\n",
    "        print i\n",
    "    sent_vect = get_sent_comp_vect(tfidf_dict, word_vects, line)\n",
    "    # new conversation\n",
    "    if sent_pos[i] < prev_sent_pos:\n",
    "        tmp_sent_vect = sent_vect\n",
    "        pos_counter = 1\n",
    "    else:\n",
    "        # add curr sent vect to previous sentences  \n",
    "        tmp_sent_vect = np.add(tmp_sent_vect * past_w, sent_vect * curr_w) / pos_counter\n",
    "        pos_counter += 1\n",
    "    tmp_sent_vect /= np.linalg.norm(tmp_sent_vect)\n",
    "    line = str(i) + ',' + ','.join(map(str, tmp_sent_vect)) + '\\n'\n",
    "    client_sentences.write(line) \n",
    "    prev_sent_pos = sent_pos[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHOWTIME, Chatty2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load sentence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client_vects = pd.read_csv('/home/daniel/Desktop/fastText/clientMessagesVectors_Labelled.csv', index_col=0, header=None)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define input cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd ~/ms/Daniel/\n",
    "import preprocessing.filtering as filtering\n",
    "reload(filtering)\n",
    "from preprocessing.filtering import NLTKPreprocessor\n",
    "from nltk.corpus import words\n",
    "data_folder = '/home/daniel/s2ds/Data/'\n",
    "# nltk_preproc = NLTKPreprocessor()\n",
    "names = filtering.build_names(data_folder)\n",
    "# list of words for spellcheck\n",
    "word_dict = {w: 1 for w in words.words()}\n",
    "contractions = {v: k for k, v in filtering.get_contractions().items()}\n",
    "word_dict.update(contractions)\n",
    "def clean_input(sentence):\n",
    "    sent_clean = filtering.filter_line(sentence, preprocessing=True, names=names, spellcheck=word_dict)\n",
    "    return sent_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make lookup sentence DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import izip\n",
    "data_folder = '/home/daniel/s2ds/Data/'\n",
    "client_file = '03_clientMessages.txt'\n",
    "client_file = codecs.open(os.path.join(data_folder, client_file), 'rU', 'utf-8')\n",
    "agent_file = '03_agentMessages.txt'\n",
    "agent_file = codecs.open(os.path.join(data_folder, agent_file), 'rU', 'utf-8')\n",
    "client_agent_dict = {}\n",
    "line_count = 0\n",
    "with client_file as client, agent_file as agent:\n",
    "        for client_line, agent_line in izip(client, agent):\n",
    "            client_line = client_line.strip()\n",
    "            agent_line = agent_line.strip()\n",
    "            client_agent_dict[line_count] = (client_line, agent_line) \n",
    "            line_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instead of KDtree, we normalize the sentence vectors to length 1 and take \n",
    "# the dotproduct with the sentence matrix and find the smallest cosine similarity\n",
    "client_norms = np.linalg.norm(client_vects.values, axis=1)\n",
    "data = client_vects.values / client_norms[:, np.newaxis]\n",
    "client_vects_norm = pd.DataFrame(data, index=client_vects.index)\n",
    "def get_closest_cosine(query_vect):\n",
    "    query_vect /= np.linalg.norm(query_vect)\n",
    "    cosines = np.dot(client_vects_norm.values, query_vect)\n",
    "    return np.nanargmax(cosines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "tree = KDTree(client_vects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "query = \"I cannot find my parcel are you sure it was delivered at the right address?\"\n",
    "#query = 'I lost my password and now I cannot log in to my account '\n",
    "#query = \"Hi could you help me with a lost order?\"\n",
    "query_clean = clean_input(query)\n",
    "print query_clean\n",
    "\n",
    "query_vect = get_sent_comp_vect(tfidf_dict, word_vects, query_clean).reshape(1, -1)\n",
    "#closest_filtered_ind = tree.query(query_vect, k=1)[1][0][0]\n",
    "closest_filtered_ind = get_closest_cosine(query_vect.T)\n",
    "# get the corresponding closest line in the unfiltered messages\n",
    "line_num = np.where(summary.linePosFiltered.values == float(closest_filtered_ind))[0][0]\n",
    "line_pos = summary.convPos.values[line_num]\n",
    "line_num = summary.linePos.values[line_num]\n",
    "print line_pos, client_agent_dict[line_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def chatbot():\n",
    "    while True:\n",
    "        query_clean = clean_input(raw_input())\n",
    "        query_vect = get_sent_comp_vect(tfidf_dict, word_vects, query_clean).reshape(1, -1)\n",
    "        closest_filtered_ind = get_closest_cosine(query_vect.T)\n",
    "        #closest_filtered_ind = tree.query(query_vect, k=1)[1][0][0]\n",
    "        # get the corresponding closest line in the unfiltered messages\n",
    "        line_num = np.where(summary.linePosFiltered.values == float(closest_filtered_ind))[0][0]\n",
    "        line_num = summary.linePos.values[line_num]\n",
    "        print client_agent_dict[line_num]\n",
    "chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit LDA to doc2vecTrainingData and print most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import codecs, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_folder = '/home/daniel/s2ds/Data/'\n",
    "input_file = '05_fastTextConv.txt'\n",
    "input_file = codecs.open(os.path.join(data_folder, input_file), 'rU', 'utf-8')\n",
    "docs = []\n",
    "for line in input_file:\n",
    "    docs.append(line.strip())\n",
    "    \n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "#lda = LatentDirichletAllocation(n_topics=21, learning_offset=20., random_state=0, n_jobs=-1, max_iter=30)\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=.01, max_features=3000, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(docs)\n",
    "#lda.fit(tf)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_top_words(lda, tf_feature_names, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(lda, '/home/daniel/Desktop/fastText/lda.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "lda = joblib.load('/home/daniel/Desktop/fastText/lda.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the topic dist for all conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 207 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit topics_dist = lda.transform(tf[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training data for fastText classifier from 05_fastTextCov.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords as sw\n",
    "# from nltk import wordpunct_tokenize\n",
    "# stopwords = set(sw.words('english'))\n",
    "output_file = codecs.open(os.path.join(data_folder, '05_fastTextConvLabelled.txt'), 'w', 'utf-8')\n",
    "for i, doc in enumerate(docs):\n",
    "    label =  ' __label__Topic' + str(np.argmax(topics_dist[i,:])) + '\\n'\n",
    "    # doc_filtered = ' '.join([w for w in wordpunct_tokenize(doc.strip()) if w not in stopwords])\n",
    "    output_file.write(doc.strip() + label)\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training data for fastText classifier from 05_clientMessagesFiltered.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_file = codecs.open(os.path.join(data_folder, '05_clientMessagesFilteredfastText.txt'), 'rU', 'utf-8')\n",
    "output_file = codecs.open(os.path.join(data_folder, '05_clientMessagesFilteredfastTextLabelled.txt'), 'w', 'utf-8')\n",
    "summary_file = os.path.join(data_folder, 'client_agent_summary3.csv')\n",
    "summary = pd.read_csv(summary_file, index_col=0)\n",
    "# get the position of each sentence in a given conversation: matches length of client_file\n",
    "sent_pos = summary.convPos[~pd.isnull(summary.linePosFiltered)].values\n",
    "for i, line in enumerate(input_file):\n",
    "    line_num = np.where(summary.linePosFiltered.values == float(i))[0]\n",
    "    conv_id = int(summary.convIDFiltered.values[line_num])\n",
    "    label =  ' __label__Topic' + str(np.argmax(topics_dist[conv_id,:])) + '\\n'\n",
    "    output_file.write(line.strip() + label)\n",
    "output_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
