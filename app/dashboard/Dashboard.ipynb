{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create merged file for dashboard\n",
    "Merge sentiment_client_agent.csv with the LDA classification output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "data_folder = '/home/daniel/s2ds/Data/'\n",
    "client_agent_file = 'sentiment_client_agent.csv'\n",
    "ms_topics_file = '05_fastTextConv_MStopicId.txt'\n",
    "conv_file = 'dashboard.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build dict with convID being the key and LDA predicted topic the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MS_topic_labels = \\\n",
    "{ 0:'Other reason'\\\n",
    ", 1:'Dropped chat'\\\n",
    ", 2:'Promotion code'\\\n",
    ", 3:'Sparks card'\\\n",
    ", 4:'Live chat'\\\n",
    ", 5:'Placing order'\\\n",
    ", 6:'Amending order'\\\n",
    ", 7:'Delivery'\\\n",
    ", 8:'Food'\\\n",
    ", 9:'Password reset'\\\n",
    ",10:'Policy queries'\\\n",
    ",11:'GM availability'\\\n",
    ",12:'Store feedback'\\\n",
    ",13:'Dropped call'\\\n",
    ",14:'GM quality'\\\n",
    ",15:'Returns/refunds'\\\n",
    ",16:'Transfer'\\\n",
    ",17:'Website feedback'\\\n",
    ",18:'Password  reset'}\n",
    "\n",
    "ms_topics_file = codecs.open(os.path.join(data_folder, ms_topics_file), 'rU', 'utf-8')\n",
    "ms_topics = {}\n",
    "\n",
    "for i, line in enumerate(ms_topics_file):\n",
    "    i\n",
    "    ms_topics[i] = MS_topic_labels[int(line.strip())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add LDA topics as new column to filtered summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary = pd.read_csv(os.path.join(data_folder, client_agent_file), index_col=0)\n",
    "sentiment = summary[['client', 'agent', 'convIDFiltered']]\n",
    "# get rid of rows/conversations which were filtered out by the preprocessing \n",
    "# and also multiple rows within the same conv (corresponding diff messages)\n",
    "filter_ind = ~pd.isnull(summary.convIDFiltered.drop_duplicates())\n",
    "# tricky indexing, didn't find a better way to do this efficiently\n",
    "summary = summary.loc[filter_ind.index[np.where(filter_ind)[0]]]\n",
    "# turn lda numbers into lda topic labels\n",
    "lda_topics = [ms_topics[convID] for convID in summary.convIDFiltered.values]\n",
    "summary.insert(summary.shape[1], 'msTopic', lda_topics)\n",
    "# discard stuff we don't need\n",
    "summary = summary[['convIDFiltered', 'convLen', 'convDate', 'convSec', 'agentID', 'msTopic']]\n",
    "# add new human readable column names\n",
    "summary.columns = ['MS_ID', 'Messages', 'Date', 'Seconds', 'AgentID', 'Topic']\n",
    "# dirty trick to get rid of 3 missing values in AgentID column\n",
    "summary.iloc[np.where(pd.isnull(summary.AgentID))[0], 4] = ['c1817483'] * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add columns for date (month, day, hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this dict holds the min, median and max of all continuous cols in summary\n",
    "min_med_max = {}\n",
    "\n",
    "# get min, med, max dates\n",
    "date = pd.to_datetime(summary.Date.values.ravel(), errors='coerce',infer_datetime_format=True)\n",
    "min_date = date.min()\n",
    "delta_date = pd.to_timedelta((date - min_date).astype('m8[ms]').to_series().median(), unit='ms')\n",
    "median_date = min_date + delta_date\n",
    "max_date = date.max()\n",
    "min_med_max['Date'] = {\n",
    "    'min': min_date,\n",
    "    'median': median_date,\n",
    "    'max': max_date\n",
    "}\n",
    "\n",
    "# replace original date var\n",
    "summary.drop('Date', 1, inplace=True)\n",
    "summary.insert(summary.shape[1], 'Date', pd.Series(date).values)\n",
    "\n",
    "# add month\n",
    "date = pd.to_datetime(summary.Date)\n",
    "months = {\n",
    "    '1': 'Jan',\n",
    "    '2': 'Feb',\n",
    "    '3': 'Mar',\n",
    "    '4': 'Apr',\n",
    "    '5': 'May',\n",
    "    '6': 'Jun',\n",
    "    '7': 'Jul',\n",
    "    '8': 'Aug',\n",
    "    '9': 'Sep',\n",
    "    '10': 'Oct',\n",
    "    '11': 'Nov',\n",
    "    '12': 'Dec',\n",
    "}\n",
    "months = [months[str(m)] for m in date.dt.month.values]\n",
    "summary.insert(summary.shape[1], 'Month', months)\n",
    "\n",
    "# add weekday\n",
    "weekdays = {\n",
    "    '0': 'Mon',\n",
    "    '1': 'Tue',\n",
    "    '2': 'Wed',\n",
    "    '3': 'Thu',\n",
    "    '4': 'Fri',\n",
    "    '5': 'Sat',\n",
    "    '6': 'Sun'\n",
    "}\n",
    "weekdays = [weekdays[str(m)] for m in date.dt.dayofweek.values]\n",
    "summary.insert(summary.shape[1], 'Day', weekdays)\n",
    "\n",
    "# add hour of conversation\n",
    "summary.insert(summary.shape[1], 'Hour', date.dt.hour.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add columns for sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# find indices of first and last messages in each conversation\n",
    "ind_first = sentiment.convIDFiltered.drop_duplicates(keep='first')\n",
    "ind_first = ind_first[~pd.isnull(ind_first)].index\n",
    "ind_last = sentiment.convIDFiltered.drop_duplicates(keep='last')\n",
    "ind_last = ind_last[~pd.isnull(ind_last)].index\n",
    "\n",
    "# get sentiment of first and last sentence\n",
    "first_sentiment = sentiment.client.loc[ind_first].values\n",
    "last_sentiment = sentiment.client.loc[ind_last].values\n",
    "\n",
    "# fit a linear line in parallel to sentiment to get an overall trend of conv sentiment\n",
    "def get_sentiment_fit(i):\n",
    "    data = sentiment[sentiment.convIDFiltered == i]\n",
    "    y = data.client\n",
    "    x = range(data.shape[0])\n",
    "    r = stats.spearmanr(x, y)[0]\n",
    "    if np.isnan(r):\n",
    "        r = 0\n",
    "    return r\n",
    "\n",
    "num_conv = sentiment.convIDFiltered.max() + 1\n",
    "sentiment_fit = np.array(Parallel(n_jobs=-1)(delayed(get_sentiment_fit)(i) for i in np.arange(num_conv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary.insert(summary.shape[1], 'SentimentFirst', first_sentiment)\n",
    "summary.insert(summary.shape[1], 'SentimentLast', last_sentiment)\n",
    "summary.insert(summary.shape[1], 'SentimentFit', sentiment_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the dataset discard very long and very rare chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary = summary[summary.Messages < 21]\n",
    "summary.Seconds = summary.Seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save file as json and make it human readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change MS_ID to int before save\n",
    "# summary.MS_ID = range(1, summary.shape[0] + 1)\n",
    "# save it as json\n",
    "json_file = '/home/daniel/ms/Daniel/dashboard/ms.json'\n",
    "summary.to_json(os.path.join(json_file), orient='records', date_format='iso')\n",
    "# make sure the json is human readable, pandas output is just one very long line\n",
    "f = open(json_file, 'r')\n",
    "overwrite = f.read().replace('},{' , '},\\n{')\n",
    "f = open(json_file, 'w')\n",
    "f.write(overwrite)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add min median and max values for continous variables and print them so we can insert it into JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cont_cols = ['Messages', 'Seconds', 'Hour', 'SentimentFirst', 'SentimentLast', 'SentimentFit']\n",
    "for col in cont_cols:\n",
    "    min_med_max[col] = {\n",
    "        'min': summary[col].min() - abs(summary[col].min()) * .05,\n",
    "        'median': summary[col].median(),\n",
    "        'max': summary[col].max() + abs(summary[col].max()) * .05\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentFit\n",
      "-1.05\n",
      "0.316227766017\n",
      "1.05\n",
      "--------------------\n",
      "Hour\n",
      "0.0\n",
      "13.0\n",
      "24.15\n",
      "--------------------\n",
      "SentimentLast\n",
      "-1.030365\n",
      "0.3818\n",
      "1.049685\n",
      "--------------------\n",
      "Seconds\n",
      "4.75\n",
      "615.0\n",
      "5610.15\n",
      "--------------------\n",
      "Messages\n",
      "0.95\n",
      "4.0\n",
      "21.0\n",
      "--------------------\n",
      "Date\n",
      "2016-04-01 06:00:56\n",
      "2016-05-02 15:27:09\n",
      "2016-06-02 14:09:41\n",
      "--------------------\n",
      "SentimentFirst\n",
      "-1.04727\n",
      "0.0\n",
      "1.049685\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for k, v in min_med_max.iteritems():\n",
    "    print k\n",
    "    print v['min']\n",
    "    print v['median']\n",
    "    print v['max']\n",
    "    print '--------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
