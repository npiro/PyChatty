{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-ocurrence statitics using CCA\n",
    "\n",
    "### To do\n",
    "- create plotting module\n",
    "- move output to data folder\n",
    "- clean notebook\n",
    "- improve file names \n",
    "    corpus_a, ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/walter/Dropbox/S2DS - M&S/Code/Walter/cca-master\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/home/walter/Dropbox/S2DS - M&S/Data\"\n",
    "code_dir = \"/home/walter/Dropbox/S2DS - M&S/Code/Walter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performing canonical correlation analysis (CCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/walter/Dropbox/S2DS - M&S/Code/Walter/cca-master\n",
      "\tProcessed 2296000 tokens\n",
      "Total 2296928 tokens\n",
      "Sorting 34228 1grams and writing to: /home/walter/Dropbox/S2DS - M&S/Data/05_agentMessagesFiltered_bigram.1grams\n",
      "Reading unigrams\n",
      "Cutoff 1: keep 18624 out of 34228 words (99.32% unigram mass)\n",
      "\t\n",
      "Creating directory /home/walter/Dropbox/S2DS - M&S/Data/05_agentMessagesFiltered_bigram.cutoff1.window10\n",
      "m: 50\n",
      "kappa: 2\n",
      "XYstats: /home/walter/Dropbox/S2DS - M&S/Data/05_agentMessagesFiltered_bigram.cutoff1.window10/XY\n",
      "Xstats: /home/walter/Dropbox/S2DS - M&S/Data/05_agentMessagesFiltered_bigram.cutoff1.window10/X\n",
      "Ystats: /home/walter/Dropbox/S2DS - M&S/Data/05_agentMessagesFiltered_bigram.cutoff1.window10/Y\n",
      "\t\n",
      "Constructing matrices\n",
      "\n",
      "Start time: 2016-08-08 19:53:29\n",
      "________________________________________\n",
      "Omega: dimensions 18625 x 167625, 5271992 nonzeros\n",
      "Computing 50 left singular vectors U of Omega...\n",
      "________________________________________\n",
      "End time: 2016-08-08 19:54:08\n",
      "Total time: 0:00:39\n",
      "\n",
      "Storing singular values at: output/05_agentMessagesFiltered_bigram.cutoff1.window10.m50.kappa2.out/sv\n",
      "Storing row-normalized U at: output/05_agentMessagesFiltered_bigram.cutoff1.window10.m50.kappa2.out/Ur\n",
      "mv: das Verschieben von »/home/walter/Dropbox/S2DS - M&S/Code/Walter/cca-master/output/05_agentMessagesFiltered_bigram.cutoff1.window10.m50.kappa2.out/“ nach »../../Data/Walter/“ ist nicht möglich: Datei oder Verzeichnis nicht gefunden\n"
     ]
    }
   ],
   "source": [
    "def run_cca(file_name):\n",
    "    window_lenght = 10\n",
    "    kappa         = 2\n",
    "    dimensions    = 50\n",
    "\n",
    "\n",
    "    file_path  = '\"' + data_dir + \"/\" + file_name + \".txt\" + '\"'\n",
    "    cca_dir    = '\"' + data_dir + \"/\" + file_name + \".cutoff1.window\" + str(window_lenght) + \"/\" + '\"'\n",
    "    svd_outdir = '\"' + code_dir + \"/cca-master/output/\" + file_name +\".cutoff1.window\"+\\\n",
    "        str(window_lenght) + \".m\" + str(dimensions) + \".kappa\" + str(kappa) + \".out/\" + '\"'\n",
    "\n",
    "    %cd $code_dir\"/cca-master\"    \n",
    "    !python cca.py --corpus $file_path --cutoff 1 --window $window_lenght;\n",
    "    !python cca.py --stat   $cca_dir   --no_matlab --m $dimensions --kappa $kappa\n",
    "    !mv -r $svd_outdir ../../Data/Walter/\n",
    "    \n",
    "#file_name     = \"agentMessages\"\n",
    "#file_name     = \"clientMessages\"\n",
    "#file_name     = \"05_clientMessagesFiltered_trigram\"\n",
    "file_name     = \"05_agentMessagesFiltered_bigram\"\n",
    "run_cca(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting CCA analysis output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "file_name     = \"clientMessages\"\n",
    "Ur_filename = svd_outdir + 'Ur'\n",
    "#plot_coocurrence_svd(file_name,Ur_filename)\n",
    "plot_coocurrence_svd(2,Ur_filename):\n",
    "\n",
    "#plt.subplot(1,2,2)\n",
    "#file_name     = \"agentMessages\"\n",
    "#Ur_filename = get_Ur_filename(code_dir,file_name,window_lenght,kappa)\n",
    "#plot_coocurrence_svd(file_name,Ur_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from StringIO import StringIO\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dimension=50\n",
    "dimensionToPlot = 5\n",
    "\n",
    "def plot_svd(file_name):\n",
    "    svFile=open(file_name,'r')\n",
    "    sValues = np.empty([dimension])\n",
    "    xRange = np.empty([dimension])\n",
    "    for i in range(dimension):\n",
    "        sValues[i]  = svFile.readline()\n",
    "        xRange[i] = i \n",
    "    plt.bar(xRange,sValues)\n",
    "    sb.plt.ylabel(\"Singular value\")\n",
    "    sb.plt.xlabel(\"Singular values order\")\n",
    "\n",
    "#plt.figure()\n",
    "#plot_svd('sv')\n",
    "\n",
    "\n",
    "#urFile=open('Ur','r')\n",
    "#testUr=np.genfromtxt(StringIO(urFile.readlines()))\n",
    "\n",
    "\n",
    "def plot_coocurrence_svd2D(file_name,Ur_filename,words_number=100):\n",
    "    urFile=open(Ur_filename,'r')\n",
    "    \n",
    "    words = list()\n",
    "    vectors = np.empty([words_number, dimensionToPlot])\n",
    "    for i in range(words_number):\n",
    "        print i\n",
    "        L1list = urFile.readline()\n",
    "        if dimension == dimensionToPlot:\n",
    "            [frequency,word, val_1, val_2] = L1list.split(' ')\n",
    "        else:\n",
    "            [frequency,word, val_1, val_2, dump] = L1list.split(' ',4) \n",
    "        print [frequency,word, val_1, val_2]\n",
    "        words.append(word.decode('utf8') )\n",
    "        vectors[i,0] = val_1\n",
    "        vectors[i,1] = val_2\n",
    "    #print words\n",
    "    #print vectors\n",
    "    plt.scatter(vectors[:,0], vectors[:,1])\n",
    "    for word, x, y in zip(words, vectors[:,0], vectors[:,1]):\n",
    "        sb.plt.annotate(word, (x, y), size=12)\n",
    "    \n",
    "    plt.title(file_name)\n",
    "    plt.autoscale(True, 'both', True)\n",
    "\n",
    "def plot_coocurrence_svd3D(file_name,Ur_filename,words_number=100):\n",
    "    urFile=open(Ur_filename,'r')\n",
    "\n",
    "    words = list()\n",
    "    vectors = np.empty([words_number, dimensionToPlot])\n",
    "    for i in range(words_number):\n",
    "        L1list = urFile.readline()\n",
    "        if dimension == dimensionToPlot:\n",
    "            [frequency,word, val_1, val_2, val_3] = L1list.split(' ')\n",
    "        else:\n",
    "            [frequency,word, val_1, val_2, val_3, dump] = L1list.split(' ',5)\n",
    "        words.append(word.decode('utf8') )\n",
    "        vectors[i,0] = val_1\n",
    "        vectors[i,1] = val_2\n",
    "        vectors[i,2] = val_3\n",
    "    #print words\n",
    "    #print vectors\n",
    "        \n",
    "    plt.scatter(vectors[:,0], vectors[:,1], 20, vectors[:,2])\n",
    "    for word, x, y in zip(words, vectors[:,0], vectors[:,1]):\n",
    "        sb.plt.annotate(word, (x, y), size=12)\n",
    "    \n",
    "    plt.title(file_name)\n",
    "    plt.autoscale(True, 'both', True)\n",
    "    \n",
    "def plot_coocurrence_svd4D(file_name,Ur_filename,words_number=100):\n",
    "    urFile=open(Ur_filename,'r')\n",
    "\n",
    "    words = list()\n",
    "    vectors = np.empty([words_number, dimensionToPlot])\n",
    "    for i in range(words_number):\n",
    "        L1list = urFile.readline()\n",
    "        if dimension == dimensionToPlot:\n",
    "            [frequency,word, val_1, val_2, val_3, val_4] = L1list.split(' ')\n",
    "        else:\n",
    "            [frequency,word, val_1, val_2, val_3, val_4, dump] = L1list.split(' ',6)\n",
    "        words.append(word.decode('utf8') )\n",
    "        vectors[i,0] = val_1\n",
    "        vectors[i,1] = val_2\n",
    "        vectors[i,2] = val_3\n",
    "        vectors[i,3] = val_4\n",
    "    #print words\n",
    "    #print vectors\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(vectors[:,0], vectors[:,1], vectors[:,3], 'z', 20, vectors[:,2])\n",
    "\n",
    "\n",
    "    for word, x, y, z in zip(words, vectors[:,0], vectors[:,1], vectors[:,3]):\n",
    "        ax.text(x, y, z, word)\n",
    "    \n",
    "    plt.title(file_name)\n",
    "    plt.autoscale(True, 'both', True)\n",
    "\n",
    "def plot_coocurrence_svd5D(file_name,Ur_filename,words_number=100):\n",
    "    urFile=open(Ur_filename,'r')\n",
    "\n",
    "    words = list()\n",
    "    vectors = np.empty([words_number, dimensionToPlot])\n",
    "    for i in range(words_number):\n",
    "        L1list = urFile.readline()\n",
    "        if dimension == dimensionToPlot:\n",
    "            [frequency,word, val_1, val_2, val_3, val_4, val_5] = L1list.split(' ')\n",
    "        else:\n",
    "            [frequency,word, val_1, val_2, val_3, val_4, val_5, dump] = L1list.split(' ',7)\n",
    "        words.append(word.decode('utf8') )\n",
    "        vectors[i,0] = val_1\n",
    "        vectors[i,1] = val_2\n",
    "        vectors[i,2] = val_3\n",
    "        vectors[i,3] = val_4\n",
    "        vectors[i,4] = val_5\n",
    "\n",
    "    #print words\n",
    "    #print vectors\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(vectors[:,0], vectors[:,1], vectors[:,3], 'z', vectors[:,4], vectors[:,2])    \n",
    "\n",
    "    for word, x, y, z in zip(words, vectors[:,0], vectors[:,1], vectors[:,3]):\n",
    "        ax.text(x, y, z, word)\n",
    "    plt.title(file_name)    \n",
    "    plt.autoscale(True, 'both', True)\n",
    "\n",
    "def plot_coocurrence_svd(dimensionToPlot,Ur_filename):\n",
    "    #plt.figure()\n",
    "    if dimensionToPlot == 2:\n",
    "        plot_coocurrence_svd2D('Word space, d=2',Ur_filename,words_number=100)\n",
    "    elif dimensionToPlot == 3:\n",
    "        plot_coocurrence_svd3D('Word space, d=3',Ur_filename,words_number=100)\n",
    "    elif dimensionToPlot == 4:\n",
    "        plot_coocurrence_svd4D('Word space, d=4',Ur_filename,words_number=100)\n",
    "    elif dimensionToPlot == 5:\n",
    "        plot_coocurrence_svd4D('Word space, d=5',Ur_filename,words_number=100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_coocurrence_svd(file_name,Ur_filename,words_number=100):\n",
    "    svFile=open(Ur_filename,'r')\n",
    "\n",
    "    words = list()\n",
    "    vectors = np.empty([words_number, 2])\n",
    "    for i in range(words_number):\n",
    "        L1list = svFile.readline()\n",
    "        [frequency,word, val_1, val_2] = L1list.split()\n",
    "        words.append(word.decode('utf8') )\n",
    "        vectors[i,0] = val_1\n",
    "        vectors[i,1] = val_2\n",
    "    #print words\n",
    "    #print vectors\n",
    "\n",
    "    plt.scatter(vectors[:,0], vectors[:,1])\n",
    "    for word, x, y in zip(words, vectors[:,0], vectors[:,1]):\n",
    "        sb.plt.annotate(word, (x, y), size=12)\n",
    "    \n",
    "    plt.title(file_name)\n",
    "\n",
    "#def get_Ur_filename(code_dir,file_name,window_lenght,kappa):\n",
    "#    return code_dir + \"/cca-master/output/\"+ file_name +\".cutoff1.window\"+\\\n",
    "#    str(window_lenght) + \".m2.kappa\" + str(kappa) + \".out/Ur\"\n",
    "\n",
    "plt.figure(num=None, figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "file_name     = \"clientMessages\"\n",
    "Ur_filename = get_Ur_filename(code_dir,file_name,window_lenght,kappa)\n",
    "plot_coocurrence_svd(file_name,Ur_filename)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "file_name     = \"agentMessages\"\n",
    "Ur_filename = get_Ur_filename(code_dir,file_name,window_lenght,kappa)\n",
    "plot_coocurrence_svd(file_name,Ur_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCA documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load README\n",
    "Author: Karl Stratos (stratos@cs.columbia.edu)\n",
    "\n",
    "Release version: 1.0\n",
    "                                      \n",
    "Requirements: python (2.7), numpy, scipy, sparsesvd, Matlab\n",
    "\n",
    "This program is an implementation of canoncial correlation analysis (CCA) in \n",
    "the context of deriving word embeddings. A theoretical justification of this \n",
    "implementation is provided in: \n",
    "\n",
    "A spectral algorithm for learning class-based n-gram models of natrual language\n",
    "Karl Stratos, Do-kyum Kim, Michael Collins, and Daniel Hsu.\n",
    "In Proceedings of UAI (2014).\n",
    "\n",
    "v------------------------------------------------------------------------------v\n",
    "| Setup                                                                        |\n",
    "^------------------------------------------------------------------------------^\n",
    "First, make sure your machine has all the required programs listed above. Also,\n",
    "to be able to run Matlab on your machine, you need to change the line in the\n",
    "call_matlab function in src/call_matlab.py to the path to Matlab on that \n",
    "machine. For example, for me it's: \n",
    "\n",
    "matlab = '/Applications/MATLAB_R2013b.app/bin/matlab' \n",
    "\n",
    "The easiest way to check everything is good is to run debug.py: \n",
    "\n",
    "$ python debug.py\n",
    "\n",
    "v------------------------------------------------------------------------------v\n",
    "| Preparing input data                                                         |\n",
    "^------------------------------------------------------------------------------^\n",
    "We assume a raw (but properly tokenized) text corpus as an input. There is no \n",
    "restriction such as 'one sentence per line'---we don't need sentence boundaries.\n",
    "But sentence boundaries can be incorporated as special tokens. For example, \n",
    "there is a toy corpus input/example/example.corpus:\n",
    "\n",
    "the dog saw the cat\n",
    "the dog barked\n",
    "the cat meowed\n",
    "\n",
    "You can put boundary markers, as in:\n",
    "\n",
    "_START_ the dog saw the cat _END_\n",
    "_START_ the dog barked _END_\n",
    "_START_ the cat meowed _END_\n",
    "\n",
    "v------------------------------------------------------------------------------v\n",
    "| Step 1: Deriving statistics                                                  |\n",
    "^------------------------------------------------------------------------------^\n",
    "In step 1, we extract co-occurrence statistics. For example, running:\n",
    "\n",
    "python cca.py --corpus input/example/example.corpus --cutoff 1\n",
    "\n",
    "will create a directory input/example/example.cutoff1.window3/ that contains \n",
    "statistics of example.corpus. The command line arguments for step 1 are \n",
    "the following:\n",
    "\n",
    "  --corpus CORPUS  count words from this corpus\n",
    "  --cutoff CUTOFF  cut off words appearing <= this number\n",
    "  --vocab VOCAB    size of the vocabulary\n",
    "  --window WINDOW  size of the sliding window\n",
    "  --want WANT      want words in this file\n",
    "  --rewrite        rewrite the (processed) corpus, not statistics\n",
    "\n",
    "In particular, you can decide the context (window)---the default is 3, i.e., \n",
    "previous/next words. You can control the size of the vocabulary by discarding \n",
    "rare words (cutoff) or using only a restricted set of vocabulary (vocab). \n",
    "\n",
    "Rare words are all replaced by a special token \"<?>\".\n",
    "\n",
    "v------------------------------------------------------------------------------v\n",
    "| Step 2: Deriving embeddings Ur                                               |\n",
    "^------------------------------------------------------------------------------^\n",
    "In step 2, we run Matlab to perform SVD on the statistics from step 1. Running:\n",
    "\n",
    "python cca.py --stat input/example/example.cutoff1.window3/ --m 2 --kappa 2\n",
    "\n",
    "will create a directory output/example.cutoff1.window3.m2.kappa2.matlab.out/\n",
    "that contains the word embedding file Ur:\n",
    "\n",
    "4 the -2.3410244894135657e-01 -9.7221193337649348e-01\n",
    "3 <?> -8.6218169891930729e-01 -5.0659916901690338e-01\n",
    "2 dog -9.3955297838817597e-01 3.4240356423657153e-01\n",
    "2 cat -9.6347323867084655e-01 2.6780462722871301e-01\n",
    "\n",
    "where the format of each line is <frequency>, <word>, <val_1>, <val_2>, ..., \n",
    "<val_m>. Also, the rows are ordered in decreasing frequency. \n",
    "\n",
    "The command line arguments for step 2 are the following:\n",
    "\n",
    "  --stat STAT      directory containing statistics\n",
    "  --m M            number of dimensions\n",
    "  --kappa KAPPA    smoothing parameter\n",
    "  --quiet          quiet mode\n",
    "  --no_matlab      do not call matlab - use python sparsesvd\n",
    "\n",
    "In particular, m is the dimensionality of CCA, and kappa is a \"pseudocount\". \n",
    "The value of kappa needs to be tuned for the given corpus. Try experimenting \n",
    "with 50, 100, 200, ... (or if your data is huge like Google Ngram, 1000, 2000, \n",
    "...) until the performance on your problem stops improving. Matlab's SVD is \n",
    "very fast, so you can try many parameter values with ease. \n",
    "\n",
    "v------------------------------------------------------------------------------v\n",
    "| Optional post processing                                                     |\n",
    "^------------------------------------------------------------------------------^\n",
    "Depending on your problem, it might be a good idea to use only the top subspace \n",
    "of your word embeddings. You can derive lower dimensional embeddings via \n",
    "principal component analysis (PCA), e.g.:\n",
    "\n",
    "python src/pca.py --embedding_file output/example.cutoff1.window3.m2.kappa2.matlab.out/Ur --pca_dim 1\n",
    "\n",
    "Now you have a file Ur.pca1 that looks like:\n",
    "\n",
    "4 the 0.906265637029\n",
    "3 <?> 0.20812022154\n",
    "2 dog -0.585143449361\n",
    "2 cat -0.529242409207\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word Counting using Collections (currently broken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "counter = collections.Counter(chats)\n",
    "print(counter.most_common())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
